{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP186oFjby2J7cPM1fftAVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zerlake/zerlake.github.io/blob/master/AI_Book_Writer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqf9BK8tIeFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEik04rp3zEs"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers torch numpy pandas tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "class TextGenerator:\n",
        "    def __init__(self):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def generate_text(self, prompt, max_length=1000):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "class BookWriterAgent:\n",
        "    def __init__(self):\n",
        "        self.text_generator = TextGenerator()\n",
        "        self.story_data = {\n",
        "            'title': '',\n",
        "            'chapters': [],\n",
        "            'characters': [],\n",
        "            'plot_points': [],\n",
        "            'world_details': {}\n",
        "        }\n",
        "\n",
        "    def set_story_title(self, title):\n",
        "        self.story_data['title'] = title\n",
        "\n",
        "    def add_character(self, name, description, role):\n",
        "        self.story_data['characters'].append({\n",
        "            'name': name,\n",
        "            'description': description,\n",
        "            'role': role,\n",
        "            'arc': []\n",
        "        })\n",
        "\n",
        "    def add_plot_point(self, chapter, description, type='major'):\n",
        "        self.story_data['plot_points'].append({\n",
        "            'chapter': chapter,\n",
        "            'description': description,\n",
        "            'type': type\n",
        "        })\n",
        "\n",
        "    def add_world_detail(self, location, description):\n",
        "        self.story_data['world_details'][location] = description\n",
        "\n",
        "    def generate_chapter(self, chapter_num, theme):\n",
        "        prompt = self._create_chapter_prompt(chapter_num, theme)\n",
        "        chapter_content = self.text_generator.generate_text(prompt, max_length=2000)\n",
        "\n",
        "        self.story_data['chapters'].append({\n",
        "            'number': chapter_num,\n",
        "            'theme': theme,\n",
        "            'content': chapter_content\n",
        "        })\n",
        "\n",
        "        return chapter_content\n",
        "\n",
        "    def _create_chapter_prompt(self, chapter_num, theme):\n",
        "        characters = ', '.join([c['name'] for c in self.story_data['characters']])\n",
        "        plot_points = [p for p in self.story_data['plot_points'] if p['chapter'] == chapter_num]\n",
        "        plot_desc = '; '.join([p['description'] for p in plot_points])\n",
        "\n",
        "        return f\"\"\"\n",
        "Write Chapter {chapter_num} of \"{self.story_data['title']}\"\n",
        "Theme: {theme}\n",
        "Characters: {characters}\n",
        "Plot points: {plot_desc}\n",
        "\n",
        "Chapter {chapter_num}:\n",
        "\"\"\"\n",
        "\n",
        "    def save_story(self, filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(self.story_data, f, indent=2)\n",
        "\n",
        "    def load_story(self, filename):\n",
        "        with open(filename, 'r') as f:\n",
        "            self.story_data = json.load(f)\n",
        "\n",
        "# Example usage\n",
        "writer = BookWriterAgent()\n",
        "\n",
        "# Set up story\n",
        "writer.set_story_title(\"The Midnight Mystery\")\n",
        "writer.add_character(\"John Smith\", \"A curious detective\", \"protagonist\")\n",
        "writer.add_character(\"Emma Wilson\", \"A mysterious client\", \"deuteragonist\")\n",
        "writer.add_plot_point(1, \"John receives a cryptic letter\")\n",
        "writer.add_world_detail(\"Victorian London\", \"Foggy streets and gaslit lamps\")\n",
        "\n",
        "# Generate first chapter\n",
        "chapter1 = writer.generate_chapter(1, \"Mystery begins\")\n",
        "print(\"\\nGenerated Chapter 1:\")\n",
        "print(chapter1)\n",
        "\n",
        "# Save story\n",
        "writer.save_story(\"midnight_mystery.json\")"
      ]
    },
    {
      "source": [
        "def generate_text(self, prompt, max_length=1000):\n",
        "    inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "    try:\n",
        "        outputs = self.model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except IndexError as e:\n",
        "        print(f\"IndexError encountered during generation or decoding: {e}\")\n",
        "        return self.tokenizer.decode(outputs[0][:self.tokenizer.vocab_size], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")\n",
        "        return \"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Yr6LqzGdI_Rl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "source": [
        "def generate_text(self, prompt, max_length=1000):\n",
        "    # ... (rest of the method) ...\n",
        "    try:\n",
        "        outputs = self.model.generate(\n",
        "            # ... (input parameters) ...\n",
        "        )\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except IndexError as e:\n",
        "        print(f\"IndexError encountered during generation or decoding: {e}\")\n",
        "        # Handle the error, e.g., return a truncated version of the text\n",
        "        return self.tokenizer.decode(outputs[0][:self.tokenizer.vocab_size], skip_special_tokens=True)\n",
        "    except Exception as e:  # Catch other potential exceptions\n",
        "        print(f\"Error during text generation: {e}\")\n",
        "        # Handle other errors appropriately"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TF0-0HBtC_gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "try:\n",
        "    outputs = self.model.generate(\n",
        "        # ... (input parameters) ...\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error during text generation: {e}\")\n",
        "    # Handle the error, e.g., log it, retry with different parameters, or return a default value"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1DS4JXLlCryK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def generate_text(self, prompt, max_length=1000):\n",
        "    # ... (rest of the method) ...\n",
        "\n",
        "    try:\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except IndexError:\n",
        "        print(\"Warning: IndexError encountered during decoding. Returning truncated text.\")\n",
        "        # Handle the error, e.g., return a truncated version of the text\n",
        "        return self.tokenizer.decode(outputs[0][:self.tokenizer.vocab_size], skip_special_tokens=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZcUbruDBB_Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def generate_chapter(self, chapter_num, theme):\n",
        "    prompt = self._create_chapter_prompt(chapter_num, theme)\n",
        "    # Reducing max_length to 1000\n",
        "    chapter_content = self.text_generator.generate_text(prompt, max_length=1000)\n",
        "    # ... (rest of the method remains the same) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "snWzJsHbAxB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "class TextGenerator:\n",
        "    def __init__(self):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model.to(self.device)\n",
        "        # Increase the maximum position embeddings to accommodate longer sequences\n",
        "        self.model.config.max_position_embeddings = 2048\n",
        "        # Update the model's positional embeddings to reflect the new size\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def generate_text(self, prompt, max_length=1000):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ecN8U0Ir-SIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "class TextGenerator:\n",
        "    def __init__(self):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model.to(self.device)\n",
        "        # Set the maximum sequence length during initialization\n",
        "        self.model.config.n_positions = 2048  # Increased to accommodate longer sequences\n",
        "        self.model.config.max_position_embeddings = 2048\n",
        "\n",
        "    def generate_text(self, prompt, max_length=1000):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "K_n-hlQm7mdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "# Install dependencies\n",
        "!pip install transformers torch numpy pandas tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "class TextGenerator:\n",
        "    def __init__(self):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def generate_text(self, prompt, max_length=1000):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "\n",
        "        # The issue is likely that the generated sequence is longer than the model's maximum sequence length.\n",
        "        # Setting max_length to a smaller value should fix the issue.\n",
        "        # But to be more sure, we adjust the config\n",
        "\n",
        "        # Note that 'n_positions' corresponds to the maximum sequence length the model can handle.\n",
        "        # Setting it to max_length makes sure the model can handle the specified length.\n",
        "        self.model.config.n_positions = max_length\n",
        "        self.model.config.max_position_embeddings = max_length\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Rest of the code remains the same\n",
        "# ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "g21QT0oB5uXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMxigmKoJcE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1pN-cOLJ2pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def generate_text(self, prompt, max_length=1000):\n",
        "    inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "    try:\n",
        "        outputs = self.model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "        try:\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        except IndexError as e:\n",
        "            print(f\"IndexError encountered during generation or decoding: {e}\")\n",
        "            # Handle the error, e.g., return a truncated version of the text\n",
        "            return self.tokenizer.decode(outputs[0][:self.tokenizer.vocab_size], skip_special_tokens=True)\n",
        "    except Exception as e:  # Catch other potential exceptions\n",
        "        print(f\"Error during text generation: {e}\")\n",
        "        # Handle other errors appropriately, e.g., log them, retry with different parameters, or return a default value\n",
        "        return \"\" # or another appropriate default value"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "37GmITjbKXly"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}